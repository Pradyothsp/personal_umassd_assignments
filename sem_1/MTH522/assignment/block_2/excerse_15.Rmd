---
output:
  html_document: default
  pdf_document: default
---
# 15. This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

```{r}
boston = read.csv("/Volumes/work/MTH522/data/Boston.csv")
head(boston)
```
### (a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.

#### Crim (per capita crime rate) and zn (proportion of residential land zoned for lots over 25,000 sq.ft)

```{r}
boston.zn <- lm(crim ~ zn, data=boston)
summary(boston.zn)
```


**Observation:**
1.  From the above summary, we can see that F-statistic is 21.1 and p-value is < 5.506e-06, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and zn.

```{r}
par(mfrow = c(2, 2))
plot(boston.zn)
```

#### Per capita crime rate(crim) and Indus (proportion of non-retail business acres per town).

```{r}
boston.indus <- lm(crim ~ indus, data=boston)
summary(boston.indus)
```

```{r}
par(mfrow = c(2, 2))
plot(boston.indus)
```


**Observation:**
1.  From the above summary, we can see that F-statistic is 99.82 and p-value is < 2.2e-16, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and indus.


#### Per capita crime rate(crim) and chas (Charles River dummy variable)

```{r}
boston.chas <- lm(crim ~ chas, data=boston)
summary(boston.chas)
```
```{r}
par(mfrow = c(2, 2))
plot(boston.chas)
```



**Observation:**
1.  The p-value of the model is 0.2094 which is greater than 0.05 and this means that the chances of having a null hypothesis are high and therefore chas is not statistically significant. 
2.  We can also see from the plot that, increase in the per capita crime rate is not effecting the change in chas. we can conclude that there is no relationship between chas and crim.

#### Per capita crime rate(crim) and nox (nitrogen oxides concentration)

```{r}
boston.nox <- lm(crim ~ nox, data=boston)
summary(boston.nox)
```

```{r}
par(mfrow = c(2, 2))
plot(boston.nox)
```


**Observations:**
1.  From the above summary, we can see that F-statistic is 108.6 and p-value is < 2.2e-16, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and nox.

#### Per capita crime rate(crim) and rm (average number of rooms per dwelling)

```{r}
boston.rm <- lm(crim ~ rm, data=boston)
summary(boston.rm)
```

```{r}
par(mfrow = c(2, 2))
plot(boston.rm)
```


**Observation:**
1.  From the above summary, we can see that F-statistic is 25.45 and p-value is < 6.347e-07, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and rm.
2.  But this influence is low because of the low R squared value of 0.04807 and Adjusted R squared value of 0.04618.

```{r}
boston.age <- lm(crim ~ age, data=boston)
summary(boston.age)
```

```{r}
par(mfrow = c(2, 2))
plot(boston.age)
```



**Observation:**
1.  From the above summary, we can see that F-statistic is 71.62 and p-value is < 2.855e-16, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and age.
2.  But influence is quite small due to the low values of the R squared value of 0.1244 and Adjusted R squared value of 0.1227.

#### Per capita crime rate(crim) and dis (weighted mean of distances to five Boston employment centers).
```{r}
boston.dis <- lm(crim ~ dis, data=boston)
summary(boston.dis)
```

```{r}
par(mfrow = c(2, 2))
plot(boston.dis)
```


**Observation:**
1.  From the above summary, we can see that F-statistic is 84.89 and p-value is < 2.2e-16, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and dis.
2.  But influence is quite small due to the low values of the R squared value of 0.1441 and Adjusted R squared value of 0.1425.

#### Per capita crime rate(crim) and rad (index of accessibility to radial highways).

```{r}
boston.rad <- lm(crim ~ rad, data=boston)
summary(boston.rad)
```

```{r}
par(mfrow = c(2, 2))
plot(boston.rad)
```



**Observation:**
1.  From the above summary, we can see that F-statistic is 323.9 and p-value is < 2.2e-16, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and rad.
2.  But influence is quite small due to the low values of the R squared value of 0.3913 and Adjusted R squared value of 0.39.

#### Per capita crime rate(crim) and tax (full-value property-tax rate per $10,000).

```{r}
boston.tax <- lm(crim ~ tax, data=boston)
summary(boston.tax)
```

```{r}
par(mfrow = c(2, 2))
plot(boston.tax)
```



**Observation:**
1.  From the above summary, we can see that F-statistic is 259.2 and p-value is < 2.2e-16, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and rad.
2.  But influence is quite small due to the low values of the R squared value of 0.3396 and Adjusted R squared value of 0.3383.

#### Per capita crime rate(crim) and ptratio (pupil-teacher ratio by town)

```{r}
boston.ptratio <- lm(crim ~ ptratio, data=boston)
summary(boston.ptratio)
```

```{r}
par(mfrow = c(2,2))
plot(boston.ptratio)
```



**Observation:**
1.  From the above summary, we can see that F-statistic is 46.25 and p-value is < 2.943e-11, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and ptratio.
2.  But influence is quite small due to the low values of the R squared value of 0.08407 and Adjusted R squared value of 0.08225.


#### Per capita crime rate(crim) and lstat (lower status of the population (percent)).
```{r}
boston.lstat <- lm(crim ~ lstat,data=boston)
summary(boston.lstat)
```

```{r}
par(mfrow = c(2,2))
plot(boston.lstat)
```



**Observation:**
1.  From the above summary, we can see that F-statistic is 132 and p-value is < 2.2e-16, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and lstat.
2.  But influence is quite small due to the low values of the R squared value of 0.2076 and Adjusted R squared value of 0.206.

#### Per capita crime rate(crim) and medv (median value of owner-occupied homes in $1000s).
```{r}
boston.medv <- lm(crim ~ medv,data = boston)
summary(boston.medv)
```

```{r}
par(mfrow = c(2,2))
plot(boston.medv)
```


**Observation:**
1.  From the above summary, we can see that F-statistic is 89.49 and p-value is < 2.2e-16, meaning the chance of having a null hypothesis (β0) is very low. So, there is a statistically significant association between crim and medv.
2.  But influence is quite small due to the low values of the R squared value of 0.1508 and Adjusted R squared value of 0.1491

### (b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0?

```{r}
boston.allvar <- lm(crim~.-X,data = boston)
summary(boston.allvar)
```



**Observations:**
1.  we can only reject the null hypothesis for “zn”, ”dis”, ”rad”, ”black” and “medv” because these predictors are fitted multiple regression model are found to be statistically significant.


### (c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regres- sion model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.

```{r}
df_coefs = data.frame("multi_coefs"=summary(boston.allvar)$coef[-1,1])
df_coefs$simple_coefs = NA
```

```{r}
for(i in row.names(df_coefs)){
  reg_model = lm(crim~eval(str2lang(i)), data=boston)
  df_coefs[row.names(df_coefs)==i, "simple_coefs"] = coef(reg_model)[2]
}
```

```{r}
plot(df_coefs$simple_coefs, df_coefs$multi_coefs, col = "red", pch =20,
     xlab="Simple Regression Coefficients", 
     ylab="Multiple Regression Coefficients", 
     main = "Relationship between Multiple regression \n and univariate regression coefficients")
```

```{r}
library(corrplot)
```

```{r}
corr <-round(cor(boston[-c(1,4)]),3)
corrplot(corr, method = "number")
```


**note:**The above figure is the Correlation between different variables of the Boston Dataset
